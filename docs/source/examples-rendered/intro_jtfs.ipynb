{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Joint Time-Frequency Scattering Introductory Example\n  1. Transform a trumpet signal\n  2. Visualize coefficients\n  3. Normalize coefficients\n  4. Feed to simple PyTorch 1D CNN\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import the necessary packages\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport torch\nimport torch.nn as nn\nfrom wavespin import TimeFrequencyScattering1D\nfrom wavespin.visuals import viz_jtfs_2d\nfrom wavespin.toolkit import normalize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate trumpet and create scattering object\nLoad trumpet, duration 2.5 seconds (sampling rate, fs=22050)\ngenerated via `librosa.load(librosa.ex('trumpet'))[0][:int(2.5*22050)]`\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "x = np.load('librosa_trumpet.npy')[:2048]\nN = x.shape[-1]\n\n# 10 temporal octaves\nJ = 9\n# 8 bandpass wavelets per octave\n# J*Q ~= 144 total temporal coefficients in first-order scattering\nQ = 8\n# scale of temporal invariance, .93 ms (2**11 [samples] / fs [samples/sec])\nT = 2**7\n# 4 frequential octaves\nJ_fr = 4\n# 2 bandpass wavelets per octave\nQ_fr = 1\n# scale of frequential invariance, F/Q == 0.5 cycle per octave\nF = 16\n# average to reduce transform size and impose freq transposition invariance\naverage_fr = True\n# return packed as dict keyed by pair names for easy inspection\nout_type = 'dict:array'\n# exclude low-energy coefficients (generally uninformative); smallest `j2` also\n# take longest to compute\npaths_exclude = {'j2': 1}\n\nconfigs = dict(J=J, shape=N, Q=Q, T=T, J_fr=J_fr, Q_fr=Q_fr, F=F,\n               average_fr=average_fr, out_type=out_type,\n               paths_exclude=paths_exclude,\n               max_pad_factor=0, max_pad_factor_fr=0)\njtfs = TimeFrequencyScattering1D(**configs, frontend='numpy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Scatter\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "Scx = jtfs(x)\n\n# print pairs and shapes\nfor pair, c in Scx.items():\n    print(c.shape, '--', pair)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "viz_jtfs_2d(jtfs, Scx, viz_coeffs=1, viz_filterbank=0, fs=22050/2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feed to simple 1D conv-net\nMinimal network\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n    def __init__(self, n_channels):\n        super().__init__()\n        self.conv = nn.Conv1d(n_channels, out_channels=32, kernel_size=3)\n        self.pool = nn.AdaptiveAvgPool1d(1)  # global avg\n        self.fc   = nn.Linear(32, 2)  # e.g. binary classification\n\n    def forward(self, x):\n        x = self.pool(self.conv(x)).squeeze(-1)  # drop time dim\n        return self.fc(x)\n\n# reinitialize in torch backend\nconfigs['out_type'] = 'array'  # pack everything into one tensor\nsct = TimeFrequencyScattering1D(**configs, frontend='torch')\nxt = torch.from_numpy(x)\nScx = sct(xt).squeeze(0)[None]  # ensure there is batch dim\n\n# drop zeroth-order, generally uninformative for audio\nScx = Scx[:, 1:]\n# channel-norm (mu=None) for 1D convs (axes=-1), log norm (log1p)\nScx = normalize(Scx, mu=None, std_axis=-1, mean_axis=-1)\n# print stats\nprint(\"{:.1f}, {:.1f} -- mean, std\".format(Scx.mean(), Scx.std()))\n\n# initialize network\nn_paths = Scx.shape[1]\nnet = Net(n_paths)\n\n# get outputs, backprop\nout = net(Scx)\nloss = out.mean()\nloss.backward()\n\n# confirm gradients\ng = net.conv.weight.grad\nprint(g.shape, \"-- Conv1D weights grad shape\")\nprint(torch.abs(g).mean(), \"-- Conv1D weights grad absolute mean\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}